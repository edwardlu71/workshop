{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network (DQN)\n",
    "\n",
    "[Edward Lu](https://github.com/edwardlu71/notebooks)\n",
    "\n",
    "### Combination of Q-Learning and Neural Network\n",
    "\n",
    "- Limitation of Q-learning: too many states in the observation space, computer is not able to handle (for example the states for GO)\n",
    "- NN is good of that, we can \n",
    "    1. take state+action as NN's input, let NN generate Q value (action value calculated from trained model)\n",
    "    2. or take state as input, let NN generate all the possible Q value from all possible actions on that state (tetris)\n",
    "- instead of adjustment on physical q table, DQN has the NN to adjust its weights in model on the input for every epoch\n",
    "- Experience replay (tetris) vs Fixed Q-targets\n",
    "    1. tetris: during each epoch, it records every action (state, reward, next state) in the whole game in memory (replay_memory), and after epoch is done the game is over, the NN randomly take the sample from this memory to contruct a batch so NN can estimate loss from the difference of state_batch and next_state_batch in the memory. Be aware all the variables are tensors.\n",
    "- further study: \n",
    "    1. policy gradients\n",
    "    2. actor/critic (but need 2 NN work together, have risk learning nothing). \n",
    "    3. Deep deterministic policy gradient: Google created algorithm to join actor-critic with DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetris\n",
    "\n",
    "[example](https://github.com/edwardlu71/tetris)\n",
    "\n",
    "- changed the environment to use pygame to run the core of the interactive game\n",
    "- changed the flat code structure to oo class\n",
    "- integrated the code from standford thesis on DQN, fixed some problems\n",
    "- enhanced experience replay with interactive interface so it not only learn from self execise but also can gain human experience (of course % is too low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import os, sys, time\n",
    "from random import random, randint, sample, shuffle, randrange\n",
    "import numpy as np\n",
    "import pygame\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "from tetris_dqn import DeepQNetwork\n",
    "import logging\n",
    "from logging.config import fileConfig\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import yaml\n",
    "\n",
    "#\n",
    "# configure directories\n",
    "#\n",
    "bindir = os.path.abspath(os.path.dirname(__file__))\n",
    "etcdir = os.path.join(bindir, \"etc\")\n",
    "libdir = os.path.join(bindir, \"lib\")\n",
    "logdir = os.path.join(bindir, \"log\")\n",
    "basename = os.path.basename(__file__)\n",
    "exename = os.path.splitext(basename)[0]\n",
    "\n",
    "logging_cfg_file = os.path.join(etcdir, exename + \".yaml\")\n",
    "with open(logging_cfg_file, 'r') as f:\n",
    "    logcfg = yaml.safe_load(f.read())\n",
    "    logging.config.dictConfig(logcfg)\n",
    "\n",
    "logger = logging.getLogger(exename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tetris:\n",
    "    # region parameters\n",
    "    piece_colors = [\n",
    "        # 0 black background\n",
    "        (0, 0, 0),\n",
    "        # 1\n",
    "        (255, 255, 0),\n",
    "        # 2\n",
    "        (153, 0, 204),\n",
    "        # 3\n",
    "        (51, 204, 51),\n",
    "        # 4\n",
    "        (255, 51, 0),\n",
    "        # 5\n",
    "        (0, 255, 255),\n",
    "        # 6\n",
    "        (255, 153, 0),\n",
    "        # 7\n",
    "        (0, 0, 255),\n",
    "        # 8 Helper color for background grid\n",
    "        (25, 25, 25)\n",
    "    ]\n",
    "    pieces = [\n",
    "        # 1\n",
    "        [[1, 1],\n",
    "         [1, 1]],\n",
    "        # 2\n",
    "        [[0, 2, 0],\n",
    "         [2, 2, 2]],\n",
    "        # 3\n",
    "        [[0, 3, 3],\n",
    "         [3, 3, 0]],\n",
    "        # 4\n",
    "        [[4, 4, 0],\n",
    "         [0, 4, 4]],\n",
    "        # 5\n",
    "        [[5, 5, 5, 5]],\n",
    "        # 6\n",
    "        [[0, 0, 6],\n",
    "         [6, 6, 6]],\n",
    "        # 7\n",
    "        [[7, 0, 0],\n",
    "         [7, 7, 7]]\n",
    "    ]\n",
    "\n",
    "    # logical variables\n",
    "    cols = 10\n",
    "    rows = 20\n",
    "\n",
    "    board = None\n",
    "    piece = None\n",
    "    piece_id = None\n",
    "    next_piece_id = None\n",
    "    current_pos = {\"x\": 0, \"y\": 0}\n",
    "    action = None\n",
    "    bag = None\n",
    "\n",
    "    # statistics\n",
    "    score = 0\n",
    "    cleared_rows = 0\n",
    "    tetrominoes = 0\n",
    "\n",
    "    # states\n",
    "    gameover = False\n",
    "    paused = False\n",
    "    automode = False\n",
    "\n",
    "    # control\n",
    "    interval = 1000  # 1 second for pygame.USEREVENT\n",
    "    key_actions = {}\n",
    "\n",
    "    # gui variables\n",
    "    screen = None  # the canvas from pygame\n",
    "    hint_cols = 8\n",
    "    cell_size = 36  # pixel\n",
    "    width = None  # pixel size of play+hint fields = (cols + hint_cols) * cell_size\n",
    "    height = None  # pixel\n",
    "    field_width = None  # pixel size of play field = cols * cell_size\n",
    "\n",
    "    default_font = None\n",
    "    font_size = 22  # cell_size*3//5\n",
    "    maxfps = 30\n",
    "    pygame_clock = None\n",
    "\n",
    "    # torch\n",
    "    model = None\n",
    "    run_mode = \"play\"  # or train\n",
    "    run_path = \"\"\n",
    "\n",
    "    # debug\n",
    "    debug = 1\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    def __init__(self, automode=False):\n",
    "        self.rows = opt.rows\n",
    "        self.cols = opt.cols\n",
    "        self.cell_size = opt.cell_size\n",
    "        self.maxfps = opt.maxfps\n",
    "        self.run_mode = opt.run_mode\n",
    "        self.run_path = opt.run_path\n",
    "        self.checkpoint_path = os.path.join(opt.train_path, \"checkpoint\", \"checkpoint.pt\")\n",
    "        self.best_path = os.path.join(opt.train_path, \"best\", \"best_model\")\n",
    "\n",
    "        self.gui = opt.gui\n",
    "        self.debug = opt.debug\n",
    "\n",
    "        # display\n",
    "        self.width = self.cell_size * (self.cols + self.hint_cols)  # play area + hint area\n",
    "        self.height = self.cell_size * self.rows\n",
    "        self.field_width = self.cell_size * self.cols\n",
    "        self.bg_grid = [[8 if x % 2 == y % 2 else 0 for x in range(self.cols)] for y in range(self.rows)]\n",
    "        self.font_size = self.cell_size * 3 // 5\n",
    "\n",
    "        # https://www.pygame.org/docs/ref/key.html\n",
    "        self.key_actions = {\n",
    "            'ESCAPE': self.quit,\n",
    "            'LEFT': lambda: self.k_move(-1),\n",
    "            'RIGHT': lambda: self.k_move(+1),\n",
    "            'DOWN': lambda: self.k_drop(),\n",
    "            'PAGEUP': lambda: self.k_speeding(+1),\n",
    "            'PAGEDOWN': lambda: self.k_speeding(-1),\n",
    "            'UP': self.k_rotate,\n",
    "            'p': self.k_toggle_pause,\n",
    "            'SPACE': self.k_fast_drop,\n",
    "            'RETURN': self.start_game,\n",
    "            'F1': self.k_toggle_automode,\n",
    "            'INSERT': lambda: self.k_debuging(+1),\n",
    "            'DELETE': lambda: self.k_debuging(-1),\n",
    "        }\n",
    "\n",
    "        self.next_piece_id = self.draw_lots()\n",
    "\n",
    "        if self.gui:\n",
    "            pygame.init()\n",
    "            # accelerate key speed, first delay, and following interval\n",
    "            pygame.key.set_repeat(250, 25)\n",
    "            self.default_font = pygame.font.Font(pygame.font.get_default_font(), self.font_size)\n",
    "            # start showing the gui\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            # We do not need mouse movement events, so we block them.\n",
    "            pygame.event.set_blocked(pygame.MOUSEMOTION)\n",
    "            self.pygame_clock = pygame.time.Clock()\n",
    "\n",
    "    def run(self):\n",
    "        self.gameover = False\n",
    "        self.paused = False\n",
    "\n",
    "        self.init_torch()\n",
    "        self.start_game()\n",
    "\n",
    "        if self.gui:\n",
    "            while True:\n",
    "                if self.gameover:\n",
    "                    self.center_msg(\"\"\"Game Over!\\nYour score: %d\\nPress enter to continue\"\"\" % self.score)\n",
    "                else:\n",
    "                    if self.paused:\n",
    "                        self.center_msg(\"Paused\")\n",
    "                    else:\n",
    "                        self.screen.fill((0, 0, 0))\n",
    "                        # draw info window\n",
    "                        # draw line to split playing area\n",
    "                        pygame.draw.line(self.screen, (255, 255, 255), (self.field_width + 1, 0),\n",
    "                                         (self.field_width + 1, self.height - 1))\n",
    "                        # display message at top left of split area\n",
    "                        self.display_msg(\"Next:\", (self.field_width + self.cell_size, self.cell_size))\n",
    "                        # draw the next stone\n",
    "                        self.draw_matrix(self.pieces[self.next_piece_id], (self.cols + 1, 2.5))\n",
    "                        # show the action advised by AI\n",
    "                        if self.action is not None:\n",
    "                            self.display_msg(\"AI: x=%d, r=%d\" % (self.action[0], self.action[1]),\n",
    "                                             (self.field_width + self.cell_size, self.cell_size * 6))\n",
    "                        # show score and cleaned rows\n",
    "                        self.display_msg(\"Score: %d\\nRows: %d\" % (self.score, self.cleared_rows),\n",
    "                                         (self.field_width + self.cell_size, self.cell_size * 7.5))\n",
    "                        bumpiness, height = self.get_bumpiness_and_height(self.board)\n",
    "                        self.display_msg(\n",
    "                            \"Hole: %d\\nbumpiness: %d\\nheight: %d\" % (self.get_holes(self.board), bumpiness, height),\n",
    "                            (self.field_width + self.cell_size, self.cell_size * 10))\n",
    "                        self.display_msg(\"Automode: %r\" % (self.automode),\n",
    "                                         (self.field_width + self.cell_size, self.cell_size * 14))\n",
    "\n",
    "                        # draw play area\n",
    "                        self.draw_matrix(self.bg_grid, (0, 0))  # draw background\n",
    "                        self.draw_matrix(self.board, (0, 0))\n",
    "                        self.draw_matrix(self.piece, tuple(self.current_pos.values()))\n",
    "\n",
    "                pygame.display.update()\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.USEREVENT + 1:\n",
    "                        self.k_drop()\n",
    "                    elif event.type == pygame.QUIT:\n",
    "                        self.quit()\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        for key in self.key_actions:\n",
    "                            if event.key == eval(\"pygame.K_\" + key):\n",
    "                                self.key_actions[key]()\n",
    "                # set maximum frame per second\n",
    "                self.pygame_clock.tick(self.maxfps)\n",
    "\n",
    "    def init_torch(self):\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        self.model = torch.load(f\"{self.run_path}/tetris\", map_location=lambda storage, loc: storage)\n",
    "        self.model.eval()\n",
    "\n",
    "    def start_game(self):\n",
    "        self.init_game()\n",
    "        self.gameover = False\n",
    "        return self.get_state_properties(self.board)\n",
    "\n",
    "    def init_game(self):\n",
    "        self.new_board()\n",
    "        self.new_piece()\n",
    "        if self.automode:\n",
    "            self.ai()\n",
    "\n",
    "        if self.gui:\n",
    "            # create an event on USEREVENT queue every 1 second\n",
    "            pygame.time.set_timer(pygame.USEREVENT + 1, self.interval)\n",
    "\n",
    "    def new_board(self):\n",
    "        self.board = [\n",
    "            [0 for x in range(self.cols)]\n",
    "            for y in range(self.rows)\n",
    "        ]\n",
    "        self.score = 0\n",
    "        self.cleared_rows = 0\n",
    "        self.tetrominoes = 0\n",
    "        self.action = None\n",
    "\n",
    "    def new_piece(self):\n",
    "        self.piece_id = self.next_piece_id\n",
    "        self.next_piece_id = self.draw_lots()\n",
    "        # copy a piece from pieces\n",
    "        self.piece = [row[:] for row in self.pieces[self.piece_id]]\n",
    "        self.current_pos = {\"x\": int(self.cols / 2 - len(self.piece[0]) / 2), \"y\": 0}\n",
    "\n",
    "        if self.check_collided(self.piece, self.current_pos):\n",
    "            self.gameover = True\n",
    "\n",
    "    # region gui actions\n",
    "    def draw_matrix(self, matrix, offset):\n",
    "        off_x, off_y = offset\n",
    "        for y, row in enumerate(matrix):\n",
    "            for x, val in enumerate(row):\n",
    "                if val:  # if not 0, a color number\n",
    "                    pygame.draw.rect(self.screen,\n",
    "                                     self.piece_colors[val],\n",
    "                                     pygame.Rect(int((off_x + x) * self.cell_size),\n",
    "                                                 int((off_y + y) * self.cell_size),\n",
    "                                                 self.cell_size,\n",
    "                                                 self.cell_size), 0)\n",
    "\n",
    "    def center_msg(self, msg):\n",
    "        for i, line in enumerate(msg.splitlines()):\n",
    "            msg_image = self.default_font.render(line, False, (255, 255, 255), (0, 0, 0))\n",
    "            msgim_center_x, msgim_center_y = msg_image.get_size()\n",
    "            msgim_center_x //= 2\n",
    "            msgim_center_y //= 2\n",
    "            self.screen.blit(msg_image,\n",
    "                             (self.width // 2 - msgim_center_x,\n",
    "                              self.height // 2 - msgim_center_y + i * (10 + self.font_size)))\n",
    "\n",
    "    def display_msg(self, msg, topleft):\n",
    "        x, y = topleft\n",
    "        for line in msg.splitlines():\n",
    "            self.screen.blit(\n",
    "                self.default_font.render(line, False, (255, 255, 255), (0, 0, 0)), (int(x), int(y)))\n",
    "            y += 10 + self.font_size\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region keybroad actions\n",
    "    def quit(self):\n",
    "        if self.gui:\n",
    "            self.center_msg(\"Exiting...\")\n",
    "            pygame.display.update()\n",
    "        sys.exit()\n",
    "\n",
    "    def k_move(self, delta_x):\n",
    "        if not self.gameover and not self.paused:\n",
    "            copy_pos = self.current_pos.copy()\n",
    "            new_x = copy_pos[\"x\"] + delta_x\n",
    "            if new_x < 0:\n",
    "                new_x = 0\n",
    "            if new_x > self.cols - len(self.piece[0]):\n",
    "                new_x = self.cols - len(self.piece[0])\n",
    "            copy_pos[\"x\"] = new_x\n",
    "            if not self.check_collided(self.piece, copy_pos):\n",
    "                self.current_pos[\"x\"] = new_x\n",
    "\n",
    "    def k_rotate(self):\n",
    "        if not self.gameover and not self.paused:\n",
    "            rotated_piece = self.rotate(self.piece)\n",
    "\n",
    "            if self.check_collided(rotated_piece, self.current_pos):\n",
    "                while self.current_pos[\"x\"] + len(rotated_piece[0]) > self.cols:\n",
    "                    self.current_pos[\"x\"] -= 1\n",
    "\n",
    "            if not self.check_collided(rotated_piece, self.current_pos):\n",
    "                self.piece = rotated_piece\n",
    "\n",
    "    def k_drop(self):\n",
    "        if not self.gameover and not self.paused:\n",
    "            if self.check_colliding(self.piece, self.current_pos):\n",
    "                self.tetrominoes += 1\n",
    "                overflow = self.truncate(self.piece, self.current_pos)\n",
    "                if overflow:\n",
    "                    self.gameover = True\n",
    "                self.merge(self.board, self.piece, self.current_pos)\n",
    "                cleared_rows = 0\n",
    "                while True:\n",
    "                    for i, row in enumerate(self.board):\n",
    "                        if 0 not in row:\n",
    "                            self.remove_row(self.board, i)\n",
    "                            cleared_rows += 1\n",
    "                            break\n",
    "                    # if no row found to be removed\n",
    "                    else:\n",
    "                        if cleared_rows > 0:\n",
    "                            self.score += self.score_formula(cleared_rows)\n",
    "                            self.cleared_rows += cleared_rows\n",
    "\n",
    "                            if not self.gui:\n",
    "                                logger.debug(f\"score {self.score} cleaned rows {self.cleared_rows}\")\n",
    "                        break\n",
    "\n",
    "                self.new_piece()\n",
    "                self.ai()\n",
    "                if opt.debug > 1:\n",
    "                    logger.debug(self.action)\n",
    "\n",
    "                # auto\n",
    "                if self.automode:\n",
    "                    self.current_pos[\"x\"] = self.action[0]\n",
    "                    for _ in range(self.action[1]):\n",
    "                        self.piece = self.rotate(self.piece)\n",
    "                    if self.check_collided(self.piece, self.current_pos) or self.check_colliding(self.piece,\n",
    "                                                                                                 self.current_pos):\n",
    "                        self.gameover = True\n",
    "                return True\n",
    "            else:\n",
    "                self.current_pos[\"y\"] += 1\n",
    "        return False\n",
    "\n",
    "    def k_fast_drop(self):\n",
    "        if not self.gameover and not self.paused:\n",
    "            while (not self.k_drop()):\n",
    "                pass\n",
    "\n",
    "    def k_speeding(self, updown):\n",
    "        if self.debug > 0:\n",
    "            if updown > 0:\n",
    "                print(f\"speeding up, interval is {self.interval} ms\")\n",
    "            else:\n",
    "                print(f\"speeding down, interval is {self.interval} ms\")\n",
    "\n",
    "        while updown > 0 and self.interval > 1:\n",
    "            self.interval = self.interval // 10\n",
    "            updown -= 1\n",
    "        while updown < 0:\n",
    "            self.interval = self.interval * 10\n",
    "            updown += 1\n",
    "        if self.gui:\n",
    "            # display. create an event on USEREVENT queue every 1 second\n",
    "            pygame.time.set_timer(pygame.USEREVENT + 1, 0)\n",
    "            pygame.time.set_timer(pygame.USEREVENT + 1, self.interval)\n",
    "\n",
    "    def k_debuging(self, updown):\n",
    "        if updown == 1:\n",
    "            self.debug += 1\n",
    "            if self.debug > 0:\n",
    "                print(f\"debug level up {self.debug}\")\n",
    "        elif updown == -1:\n",
    "            if self.debug > 0:\n",
    "                self.debug -= 1\n",
    "                print(f\"debug level down {self.debug}\")\n",
    "\n",
    "    def k_toggle_pause(self):\n",
    "        self.paused = not self.paused\n",
    "        if self.debug > 0:\n",
    "            print(f\"paused? {self.paused}\")\n",
    "\n",
    "    def k_toggle_automode(self):\n",
    "        self.automode = not self.automode\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region static methods\n",
    "    @staticmethod\n",
    "    def rotate(piece):\n",
    "        # this is counterclockwise!\n",
    "        return [[piece[y][x] for y in range(len(piece))] for x in range(len(piece[0]) - 1, -1, -1)]\n",
    "        # this is clockwise\n",
    "        # return [[piece[y][x] for y in range(len(piece) - 1, -1, -1)] for x in range(len(piece[0]))]\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(board, piece, pos, inplace=True):\n",
    "        if not inplace:\n",
    "            # make a copy of recent board\n",
    "            board = [x[:] for x in board]\n",
    "        for y in range(len(piece)):\n",
    "            for x in range(len(piece[y])):\n",
    "                if piece[y][x] and not board[y + pos[\"y\"]][x + pos[\"x\"]]:\n",
    "                    board[y + pos[\"y\"]][x + pos[\"x\"]] = piece[y][x]\n",
    "        return board\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region general functions\n",
    "    def score_formula(self, cleared_rows):\n",
    "        return 1 + (cleared_rows ** 2) * self.cols\n",
    "        # return 1 + cleared_rows * self.cols - delta_holes * self.cols // 2\n",
    "\n",
    "    # check whether one more drop will collide or not\n",
    "    def check_colliding(self, piece, pos):\n",
    "        future_y = pos[\"y\"] + 1\n",
    "        for y in range(len(piece)):\n",
    "            for x in range(len(piece[y])):\n",
    "                if future_y + y > self.rows - 1 \\\n",
    "                        or pos[\"x\"] + x > self.cols - 1 \\\n",
    "                        or pos[\"x\"] + x < 0 \\\n",
    "                        or self.board[future_y + y][pos[\"x\"] + x] and piece[y][x]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # check whether the piece has already collided or not\n",
    "    def check_collided(self, piece, pos):\n",
    "        for y in range(len(piece)):\n",
    "            for x in range(len(piece[y])):\n",
    "                if pos[\"y\"] + y > self.rows - 1 \\\n",
    "                        or pos[\"x\"] + x > self.cols - 1 \\\n",
    "                        or pos[\"x\"] + x < 0 \\\n",
    "                        or self.board[pos[\"y\"] + y][pos[\"x\"] + x] and piece[y][x]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def remove_row(self, board, row):\n",
    "        del board[row]\n",
    "        board.insert(0, [0 for _ in range(self.cols)])\n",
    "\n",
    "    def remove_rows(self, board, indices):\n",
    "        for i in indices[::-1]:\n",
    "            self.remove_row(board, i)\n",
    "\n",
    "    def draw_lots(self):\n",
    "        if opt.cheating:\n",
    "            # cheating version\n",
    "            if self.bag is None or not len(self.bag):\n",
    "                self.bag = list(range(len(self.pieces)))\n",
    "                shuffle(self.bag)\n",
    "            return self.bag.pop()\n",
    "        else:\n",
    "            # self.bag = list(range(len(self.pieces)))\n",
    "            # shuffle(self.bag)\n",
    "            return randrange(len(self.pieces))\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region torch\n",
    "    def ai(self):\n",
    "        # dict of (move, rotation) => tensor([cleaned_rows, holes, bumpiness, height])\n",
    "        next_steps = self.get_next_states()\n",
    "        # next_actions: tuple of tuples of (move, rotation)\n",
    "        # next_states: list of tensors of states\n",
    "        next_actions, next_states = zip(*next_steps.items())\n",
    "        # turn a list of tensors into one tensor\n",
    "        next_states = torch.stack(next_states)\n",
    "        # use model to calculate gradients of next_states\n",
    "        predictions = self.model(next_states)[:, 0]\n",
    "        # find the maximum gradient\n",
    "        index = torch.argmax(predictions).item()\n",
    "        # take the action that gradient is corresponding to\n",
    "        self.action = next_actions[index]\n",
    "        if self.debug > 1:\n",
    "            print(f\"advised action: {self.action}\\n\")\n",
    "        if self.debug > 2:\n",
    "            print(\"piece:\")\n",
    "            [print(i, row) for i, row in enumerate(self.piece)]\n",
    "            print(\"board:\")\n",
    "            [print(i, row) for i, row in enumerate(self.board)]\n",
    "\n",
    "    def get_next_states(self):\n",
    "        states = {}\n",
    "        piece_id = self.piece_id\n",
    "        # make a copy of the piece on board\n",
    "        copy_piece = [row[:] for row in self.piece]\n",
    "\n",
    "        # different piece has different rotation possibilities\n",
    "        if piece_id == 0:  # O piece\n",
    "            num_rotations = 1\n",
    "        elif piece_id == 2 or piece_id == 3 or piece_id == 4:\n",
    "            num_rotations = 2\n",
    "        else:\n",
    "            num_rotations = 4\n",
    "\n",
    "        for i in range(num_rotations):\n",
    "            valid_xs = self.cols - len(copy_piece[0])\n",
    "            for x in range(valid_xs + 1):\n",
    "                # make a copy of copy_piece\n",
    "                piece = [row[:] for row in copy_piece]\n",
    "                pos = {\"x\": x, \"y\": 0}\n",
    "                while not self.check_colliding(piece, pos):\n",
    "                    pos[\"y\"] += 1\n",
    "                self.truncate(piece, pos)\n",
    "                # create a copy of self.board and join the this piece\n",
    "                board = self.merge(self.board, piece, pos, inplace=False)\n",
    "                # save a state for each of possible x position and rotated position of the stone after dropped\n",
    "                # calculate removed rows, holes, bumpiness and height, get torch tensor value\n",
    "                states[(x, i)] = self.get_state_properties(board)\n",
    "                if self.debug > 3:\n",
    "                    print(x, i, states[(x, i)])\n",
    "            # rotating copy_piece once\n",
    "            copy_piece = self.rotate(copy_piece)\n",
    "        if self.debug > 2:\n",
    "            print(\"x, rotate, rows_cleared, holes, bumpiness, height\")\n",
    "        return states\n",
    "\n",
    "    def get_state_properties(self, board):\n",
    "        rows_cleared, board = self.check_cleared_rows(board)\n",
    "        holes = self.get_holes(board)\n",
    "        bumpiness, height = self.get_bumpiness_and_height(board)\n",
    "        return torch.FloatTensor([rows_cleared, holes, bumpiness, height])\n",
    "\n",
    "    def check_cleared_rows(self, board):\n",
    "        to_delete = []\n",
    "        for i, row in enumerate(board[::-1]):\n",
    "            if 0 not in row:\n",
    "                to_delete.append(len(board) - 1 - i)\n",
    "        if len(to_delete) > 0:\n",
    "            self.remove_rows(board, to_delete)\n",
    "        return len(to_delete), board\n",
    "\n",
    "    def get_bumpiness_and_height(self, board):\n",
    "        board = np.array(board)\n",
    "        mask = board != 0\n",
    "        invert_heights = np.where(mask.any(axis=0), np.argmax(mask, axis=0), self.rows)\n",
    "        heights = self.rows - invert_heights\n",
    "        total_height = np.sum(heights)\n",
    "        currs = heights[:-1]\n",
    "        nexts = heights[1:]\n",
    "        diffs = np.abs(currs - nexts)\n",
    "        total_bumpiness = np.sum(diffs)\n",
    "        return total_bumpiness, total_height\n",
    "\n",
    "    def get_holes(self, board):\n",
    "        num_holes = 0\n",
    "        for col in zip(*board):\n",
    "            row = 0\n",
    "            while row < self.rows and col[row] == 0:\n",
    "                row += 1\n",
    "            num_holes += len([x for x in col[row + 1:] if x == 0])\n",
    "        return num_holes\n",
    "\n",
    "    def truncate(self, piece, pos):\n",
    "        gameover = False\n",
    "        last_collision_row = -1\n",
    "        for y in range(len(piece)):\n",
    "            for x in range(len(piece[y])):\n",
    "                if self.board[pos[\"y\"] + y][pos[\"x\"] + x] and piece[y][x]:\n",
    "                    if y > last_collision_row:\n",
    "                        last_collision_row = y\n",
    "\n",
    "        if pos[\"y\"] - (len(piece) - last_collision_row) < 0 and last_collision_row > -1:\n",
    "            while last_collision_row >= 0 and len(piece) > 1:\n",
    "                gameover = True\n",
    "                last_collision_row = -1\n",
    "                del piece[0]\n",
    "                for y in range(len(piece)):\n",
    "                    for x in range(len(piece[y])):\n",
    "                        if self.board[pos[\"y\"] + y][pos[\"x\"] + x] and piece[y][x] and y > last_collision_row:\n",
    "                            last_collision_row = y\n",
    "        return gameover\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region no gui\n",
    "    def test(self):\n",
    "        self.automode = True\n",
    "        self.init_torch()\n",
    "        while True:\n",
    "            self.start_game()\n",
    "            while not self.gameover:\n",
    "                self.ai()\n",
    "                self.step(self.action)\n",
    "            logger.info(f\"score {self.score}, cleaned rows {self.cleared_rows}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        x, num_rotations = action\n",
    "        self.current_pos = {\"x\": x, \"y\": 0}\n",
    "        for _ in range(num_rotations):\n",
    "            self.piece = self.rotate(self.piece)\n",
    "\n",
    "        while not self.check_colliding(self.piece, self.current_pos):\n",
    "            self.current_pos[\"y\"] += 1\n",
    "\n",
    "        overflow = self.truncate(self.piece, self.current_pos)\n",
    "        if overflow:\n",
    "            self.gameover = True\n",
    "\n",
    "        self.board = self.merge(self.board, self.piece, self.current_pos)\n",
    "\n",
    "        cleared_rows, self.board = self.check_cleared_rows(self.board)\n",
    "        score = self.score_formula(cleared_rows)\n",
    "        self.score += score\n",
    "        self.cleared_rows += cleared_rows\n",
    "        if not self.gameover:\n",
    "            self.new_piece()\n",
    "        if self.gameover:\n",
    "            self.score -= 2\n",
    "\n",
    "        if self.gui:\n",
    "            if not self.paused:\n",
    "                self.display()\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    self.quit()\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    for key in self.key_actions:\n",
    "                        if event.key == eval(\"pygame.K_\" + key):\n",
    "                            self.key_actions[key]()\n",
    "            # set maximum frame per second\n",
    "            # self.pygame_clock.tick(self.maxfps)\n",
    "\n",
    "        return score, self.gameover\n",
    "\n",
    "    def training_report(self):\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        model = DeepQNetwork()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "        # self.model = torch.load(f\"{self.run_path}/tetris\", map_location=lambda storage, loc: storage)\n",
    "        # self.model.eval()\n",
    "\n",
    "        # '''\n",
    "        if os.path.isfile(self.checkpoint_path):\n",
    "            model, optimizer, epoch, max_score, best_cleaned_rows, best_epoch = self.load_checkpoint(model, optimizer)\n",
    "            logger.info(\n",
    "                f\"loaded checkpoint before epoch {epoch} with historical record of max score {max_score} cleaned rows {best_cleaned_rows} on epoch {best_epoch}\")\n",
    "        # '''\n",
    "\n",
    "        from torchvision import models\n",
    "        from torchsummary import summary\n",
    "        vgg = models.vgg16()\n",
    "        summary(vgg, (3, 224, 224))\n",
    "\n",
    "    def training(self):\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        writer = SummaryWriter(os.path.join(opt.log_path, \"tensorboard\"))\n",
    "\n",
    "        epoch = 0\n",
    "        max_score = 0\n",
    "        best_cleaned_rows = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        model = DeepQNetwork()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "        if opt.cleanup:\n",
    "            self.cleanup_checkpoint()\n",
    "        if os.path.isfile(self.checkpoint_path):\n",
    "            model, optimizer, epoch, max_score, best_cleaned_rows, best_epoch = self.load_checkpoint(model, optimizer)\n",
    "            logger.info(\n",
    "                f\"loaded checkpoint before epoch {epoch} with historical record of max score {max_score} cleaned rows {best_cleaned_rows} on epoch {best_epoch}\")\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        state = self.start_game()\n",
    "        replay_memory = deque(maxlen=opt.replay_memory_size)\n",
    "        while epoch < opt.num_epochs:\n",
    "            next_steps = self.get_next_states()\n",
    "            # Exploration or exploitation\n",
    "            epsilon = opt.final_epsilon + (max(opt.num_decay_epochs - epoch, 0) * (\n",
    "                    opt.initial_epsilon - opt.final_epsilon) / opt.num_decay_epochs)\n",
    "            u = random()\n",
    "            random_action = u <= epsilon\n",
    "            next_actions, next_states = zip(*next_steps.items())\n",
    "            next_states = torch.stack(next_states)\n",
    "            # notify all your layers that you are in eval mode, that way,\n",
    "            # batchnorm or dropout layers will work in eval mode instead of training mode\n",
    "            model.eval()\n",
    "            # no_grad() impacts the autograd engine and deactivate it\n",
    "            # here it helps saving some memory for performace consideration\n",
    "            with torch.no_grad():\n",
    "                predictions = model(next_states)[:, 0]\n",
    "            # change back to train mode\n",
    "            model.train()\n",
    "            if random_action:\n",
    "                index = randint(0, len(next_steps) - 1)\n",
    "            else:\n",
    "                index = torch.argmax(predictions).item()\n",
    "\n",
    "            next_state = next_states[index, :]\n",
    "            action = next_actions[index]\n",
    "\n",
    "            reward, done = self.step(action)\n",
    "\n",
    "            # recording rewards from recent state (rows_cleared, holes, bumpiness, height) to next state\n",
    "            replay_memory.append([state, reward, next_state, done])\n",
    "            if done:\n",
    "                final_score = self.score\n",
    "                final_tetrominoes = self.tetrominoes\n",
    "                final_cleared_rows = self.cleared_rows\n",
    "                final_bumpiness, final_height = self.get_bumpiness_and_height(self.board)\n",
    "                state = self.start_game()\n",
    "            else:\n",
    "                state = next_state\n",
    "                continue\n",
    "            if len(replay_memory) < opt.replay_memory_size / 10:\n",
    "                continue\n",
    "            epoch += 1\n",
    "\n",
    "            # randomly taking opt.batch_size of samples from the recorded history\n",
    "            batch = sample(replay_memory, min(len(replay_memory), opt.batch_size))\n",
    "            state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "            state_batch = torch.stack(tuple(state for state in state_batch))\n",
    "            reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n",
    "            next_state_batch = torch.stack(tuple(state for state in next_state_batch))\n",
    "\n",
    "            # q values (512x1) from states (512x4)\n",
    "            q_values = model(state_batch)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                next_prediction_batch = model(next_state_batch)\n",
    "            model.train()\n",
    "            # calculate the predicated q values of next states from the model and reward\n",
    "            q_predicates = torch.cat(\n",
    "                tuple(reward if done else reward + opt.gamma * prediction for reward, done, prediction in\n",
    "                      zip(reward_batch, done_batch, next_prediction_batch)))[:, None]\n",
    "            # loss function (least squares),\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(q_values, q_predicates) # single float\n",
    "            loss.backward() # dLoss/dWeight\n",
    "            optimizer.step() # adjust weight\n",
    "\n",
    "            is_best = False\n",
    "            if final_score > max_score:\n",
    "                max_score = final_score\n",
    "                best_cleaned_rows = final_cleared_rows\n",
    "                best_epoch = epoch\n",
    "                is_best = True\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'max_score': max_score,\n",
    "                'best_cleaned_rows': best_cleaned_rows,\n",
    "                'best_epoch': best_epoch\n",
    "            }\n",
    "\n",
    "            self.save_checkpoint(checkpoint, model, is_best)\n",
    "\n",
    "            logger.info(\n",
    "                \"Epoch: {}/{}, Loss: {}, Score: {}, Tetrominoes {}, Cleared Rows: {}, record high: {} {} {}\".format(\n",
    "                    epoch,\n",
    "                    opt.num_epochs,\n",
    "                    loss.item(),\n",
    "                    final_score,\n",
    "                    final_tetrominoes,\n",
    "                    final_cleared_rows,\n",
    "                    max_score,\n",
    "                    best_cleaned_rows,\n",
    "                    best_epoch))\n",
    "            writer.add_scalar('Train/Score', final_score, epoch - 1)\n",
    "            writer.add_scalar('Train/Tetrominoes', final_tetrominoes, epoch - 1)\n",
    "            writer.add_scalar('Train/Cleared_Rows', final_cleared_rows, epoch - 1)\n",
    "            writer.add_scalar('Train/Bumpiness', final_bumpiness, epoch - 1)\n",
    "            writer.add_scalar('Train/Height', final_height, epoch - 1)\n",
    "            writer.add_scalar('Train/Loss', loss.item(), epoch - 1)\n",
    "\n",
    "            # if epoch > 0 and epoch % opt.save_interval == 0:\n",
    "            # torch.save(model, \"{}/tetris_{}\".format(opt.train_path, epoch))\n",
    "        torch.save(model, \"{}/tetris\".format(opt.train_path))\n",
    "\n",
    "    def save_checkpoint(self, checkpoint, model, is_best):\n",
    "        torch.save(checkpoint, self.checkpoint_path)\n",
    "        if is_best:\n",
    "            torch.save(model, self.best_path)\n",
    "\n",
    "    def load_checkpoint(self, model, optimizer):\n",
    "        checkpoint = torch.load(self.checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return model, optimizer, checkpoint['epoch'], checkpoint['max_score'], checkpoint['best_cleaned_rows'], \\\n",
    "               checkpoint['best_epoch']\n",
    "\n",
    "    def cleanup_checkpoint(self):\n",
    "        if os.path.isfile(self.checkpoint_path):\n",
    "            os.remove(self.checkpoint_path)\n",
    "        if os.path.isfile(self.best_path):\n",
    "            os.remove(self.best_path)\n",
    "\n",
    "    def display(self):\n",
    "        if self.gameover:\n",
    "            self.center_msg(\"\"\"Game Over!\\nYour score: %d\\nPress enter to continue\"\"\" % self.score)\n",
    "        else:\n",
    "            if False:  # self.paused:\n",
    "                self.center_msg(\"Paused\")\n",
    "            else:\n",
    "                self.screen.fill((0, 0, 0))\n",
    "                # draw info window\n",
    "                # draw line to split playing area\n",
    "                pygame.draw.line(self.screen, (255, 255, 255), (self.field_width + 1, 0),\n",
    "                                 (self.field_width + 1, self.height - 1))\n",
    "                # display message at top left of split area\n",
    "                self.display_msg(\"Next:\", (self.field_width + self.cell_size, 2))\n",
    "                # draw the next stone\n",
    "                self.draw_matrix(self.pieces[self.next_piece_id], (self.cols + 1, 1))\n",
    "                # show the action advised by AI\n",
    "                if self.action is not None:\n",
    "                    self.display_msg(\"Action: x=%d, rotation=%d\" % (self.action[0], self.action[1]),\n",
    "                                     (self.field_width + self.cell_size, self.cell_size * 4))\n",
    "                # show score and cleaned rows\n",
    "                self.display_msg(\"Score: %d\\nRows: %d\" % (self.score, self.cleared_rows),\n",
    "                                 (self.field_width + self.cell_size, self.cell_size * 5))\n",
    "                bumpiness, height = self.get_bumpiness_and_height(self.board)\n",
    "                self.display_msg(\n",
    "                    \"Hole: %d\\nbumpiness: %d\\nheight: %d\" % (self.get_holes(self.board), bumpiness, height),\n",
    "                    (self.field_width + self.cell_size, self.cell_size * 8))\n",
    "                self.display_msg(\"Automode: %r\" % (self.automode),\n",
    "                                 (self.field_width + self.cell_size, self.cell_size * 11))\n",
    "\n",
    "                # draw play area\n",
    "                self.draw_matrix(self.bg_grid, (0, 0))  # draw background\n",
    "                self.draw_matrix(self.board, (0, 0))\n",
    "                self.draw_matrix(self.piece, tuple(self.current_pos.values()))\n",
    "        pygame.display.update()\n",
    "    # endregion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Tetris:    \n",
    "    def step(self, action):\n",
    "        x, num_rotations = action\n",
    "        self.current_pos = {\"x\": x, \"y\": 0}\n",
    "        for _ in range(num_rotations):\n",
    "            self.piece = self.rotate(self.piece)\n",
    "\n",
    "        while not self.check_colliding(self.piece, self.current_pos):\n",
    "            self.current_pos[\"y\"] += 1\n",
    "\n",
    "        overflow = self.truncate(self.piece, self.current_pos)\n",
    "        if overflow:\n",
    "            self.gameover = True\n",
    "\n",
    "        self.board = self.merge(self.board, self.piece, self.current_pos)\n",
    "\n",
    "        cleared_rows, self.board = self.check_cleared_rows(self.board)\n",
    "        score = self.score_formula(cleared_rows)\n",
    "        self.score += score\n",
    "        self.cleared_rows += cleared_rows\n",
    "        if not self.gameover:\n",
    "            self.new_piece()\n",
    "        if self.gameover:\n",
    "            self.score -= 2\n",
    "\n",
    "        if self.gui:\n",
    "            if not self.paused:\n",
    "                self.display()\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    self.quit()\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    for key in self.key_actions:\n",
    "                        if event.key == eval(\"pygame.K_\" + key):\n",
    "                            self.key_actions[key]()\n",
    "            # set maximum frame per second\n",
    "            # self.pygame_clock.tick(self.maxfps)\n",
    "\n",
    "        return score, self.gameover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Tetris:\n",
    "    def training(self):\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        writer = SummaryWriter(os.path.join(opt.log_path, \"tensorboard\"))\n",
    "\n",
    "        epoch = 0\n",
    "        max_score = 0\n",
    "        best_cleaned_rows = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        model = DeepQNetwork()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "        if opt.cleanup:\n",
    "            self.cleanup_checkpoint()\n",
    "        if os.path.isfile(self.checkpoint_path):\n",
    "            model, optimizer, epoch, max_score, best_cleaned_rows, best_epoch = self.load_checkpoint(model, optimizer)\n",
    "            logger.info(\n",
    "                f\"loaded checkpoint before epoch {epoch} with historical record of max score {max_score} cleaned rows {best_cleaned_rows} on epoch {best_epoch}\")\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        state = self.start_game()\n",
    "        replay_memory = deque(maxlen=opt.replay_memory_size)\n",
    "        while epoch < opt.num_epochs:\n",
    "            next_steps = self.get_next_states()\n",
    "            # Exploration or exploitation\n",
    "            epsilon = opt.final_epsilon + (max(opt.num_decay_epochs - epoch, 0) * (\n",
    "                    opt.initial_epsilon - opt.final_epsilon) / opt.num_decay_epochs)\n",
    "            u = random()\n",
    "            random_action = u <= epsilon\n",
    "            next_actions, next_states = zip(*next_steps.items())\n",
    "            next_states = torch.stack(next_states)\n",
    "            # notify all your layers that you are in eval mode, that way,\n",
    "            # batchnorm or dropout layers will work in eval mode instead of training mode\n",
    "            model.eval()\n",
    "            # no_grad() impacts the autograd engine and deactivate it\n",
    "            # here it helps saving some memory for performace consideration\n",
    "            with torch.no_grad():\n",
    "                predictions = model(next_states)[:, 0]\n",
    "            # change back to train mode\n",
    "            model.train()\n",
    "            if random_action:\n",
    "                index = randint(0, len(next_steps) - 1)\n",
    "            else:\n",
    "                index = torch.argmax(predictions).item()\n",
    "\n",
    "            next_state = next_states[index, :]\n",
    "            action = next_actions[index]\n",
    "\n",
    "            reward, done = self.step(action)\n",
    "\n",
    "            # recording rewards from recent state (rows_cleared, holes, bumpiness, height) to next state\n",
    "            replay_memory.append([state, reward, next_state, done])\n",
    "            if done:\n",
    "                final_score = self.score\n",
    "                final_tetrominoes = self.tetrominoes\n",
    "                final_cleared_rows = self.cleared_rows\n",
    "                final_bumpiness, final_height = self.get_bumpiness_and_height(self.board)\n",
    "                state = self.start_game()\n",
    "            else:\n",
    "                state = next_state\n",
    "                continue\n",
    "            if len(replay_memory) < opt.replay_memory_size / 10:\n",
    "                continue\n",
    "            epoch += 1\n",
    "\n",
    "            # randomly taking opt.batch_size of samples from the recorded history\n",
    "            batch = sample(replay_memory, min(len(replay_memory), opt.batch_size))\n",
    "            state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "            state_batch = torch.stack(tuple(state for state in state_batch))\n",
    "            reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n",
    "            next_state_batch = torch.stack(tuple(state for state in next_state_batch))\n",
    "\n",
    "            # q values (512x1) from states (512x4)\n",
    "            q_values = model(state_batch)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                next_prediction_batch = model(next_state_batch)\n",
    "            model.train()\n",
    "            # calculate the predicated q values of next states from the model and reward\n",
    "            q_predicates = torch.cat(\n",
    "                tuple(reward if done else reward + opt.gamma * prediction for reward, done, prediction in\n",
    "                      zip(reward_batch, done_batch, next_prediction_batch)))[:, None]\n",
    "            # loss function (least squares),\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(q_values, q_predicates) # single float\n",
    "            loss.backward() # dLoss/dWeight\n",
    "            optimizer.step() # adjust weight\n",
    "\n",
    "            is_best = False\n",
    "            if final_score > max_score:\n",
    "                max_score = final_score\n",
    "                best_cleaned_rows = final_cleared_rows\n",
    "                best_epoch = epoch\n",
    "                is_best = True\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'max_score': max_score,\n",
    "                'best_cleaned_rows': best_cleaned_rows,\n",
    "                'best_epoch': best_epoch\n",
    "            }\n",
    "\n",
    "            self.save_checkpoint(checkpoint, model, is_best)\n",
    "\n",
    "            logger.info(\n",
    "                \"Epoch: {}/{}, Loss: {}, Score: {}, Tetrominoes {}, Cleared Rows: {}, record high: {} {} {}\".format(\n",
    "                    epoch,\n",
    "                    opt.num_epochs,\n",
    "                    loss.item(),\n",
    "                    final_score,\n",
    "                    final_tetrominoes,\n",
    "                    final_cleared_rows,\n",
    "                    max_score,\n",
    "                    best_cleaned_rows,\n",
    "                    best_epoch))\n",
    "            writer.add_scalar('Train/Score', final_score, epoch - 1)\n",
    "            writer.add_scalar('Train/Tetrominoes', final_tetrominoes, epoch - 1)\n",
    "            writer.add_scalar('Train/Cleared_Rows', final_cleared_rows, epoch - 1)\n",
    "            writer.add_scalar('Train/Bumpiness', final_bumpiness, epoch - 1)\n",
    "            writer.add_scalar('Train/Height', final_height, epoch - 1)\n",
    "            writer.add_scalar('Train/Loss', loss.item(), epoch - 1)\n",
    "\n",
    "            # if epoch > 0 and epoch % opt.save_interval == 0:\n",
    "            # torch.save(model, \"{}/tetris_{}\".format(opt.train_path, epoch))\n",
    "        torch.save(model, \"{}/tetris\".format(opt.train_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\"\"\"AI Powered Tetris\"\"\")\n",
    "    parser.add_argument(\"--cols\", type=int, default=10, help=\"columns. default:10\")\n",
    "    parser.add_argument(\"--rows\", type=int, default=20, help=\"rows. default:20\")\n",
    "    # display\n",
    "    parser.add_argument(\"--cell_size\", type=int, default=30, help=\"size of a cell. default:30\")\n",
    "    parser.add_argument(\"--maxfps\", type=int, default=30, help=\"cap the fps maximum at. default:30\")\n",
    "\n",
    "    parser.add_argument(\"--cleanup\", type=bool, default=False,\n",
    "                        help=\"True|False. clean up training history. Default:False\")\n",
    "    parser.add_argument(\"--log_path\", type=str, default=\"log\", help=\"Default:log\")\n",
    "    parser.add_argument(\"--train_path\", type=str, default=\"models/training\", help=\"Default: models/training\")\n",
    "    parser.add_argument(\"--run_path\", type=str, default=\"models\", help=\"Default: models\")\n",
    "    parser.add_argument(\"--cheating\", type=bool, default=True,\n",
    "                        help=\"use shuffle bag instead of full random piece. default:False\")\n",
    "    # run modes\n",
    "    parser.add_argument(\"--run_mode\", type=str, default=\"play\", help=\"play|train|test|report. default:play\")\n",
    "    parser.add_argument(\"--gui\", type=bool, default=True, help=\"default:True\")\n",
    "    parser.add_argument(\"--debug\", type=int, default=0, help=\"default:0\")\n",
    "\n",
    "    # torch arguments\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"The number of images per batch\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--initial_epsilon\", type=float, default=1)\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--num_decay_epochs\", type=float, default=2000)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=3000)\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1, help=\"Default:1\")\n",
    "    parser.add_argument(\"--replay_memory_size\", type=int, default=30000,\n",
    "                        help=\"Default: 30000. Number of epoches between testing phases\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    opt = get_args()\n",
    "    if opt.run_mode == \"play\":\n",
    "        opt.gui = True\n",
    "        App = Tetris()\n",
    "        App.run()\n",
    "    elif opt.run_mode == \"test\":\n",
    "        App = Tetris(automode=True)\n",
    "        if opt.gui:\n",
    "            App.k_speeding(3)\n",
    "            App.automode = True\n",
    "            App.run()\n",
    "        else:\n",
    "            App.test()\n",
    "        [print(row) for row in App.board]\n",
    "    elif opt.run_mode == \"train\":\n",
    "        App = Tetris(automode=True)\n",
    "        App.training()\n",
    "    elif opt.run_mode == \"report\":\n",
    "        opt.gui = False\n",
    "        App = Tetris(automode=True)\n",
    "        App.training_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "\n",
    "        # input layer (4 features = rows_cleared, holes, bumpiness, height)\n",
    "        # + the linear input layer (64) + ReLu (64)\n",
    "        self.conv1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n",
    "        # the hidden linear layer (64) + ReLu (64)\n",
    "        self.conv2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
    "        # the fully connected layer (64) + output layer (loss)\n",
    "        self.conv3 = nn.Sequential(nn.Linear(64, 1))\n",
    "\n",
    "        self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
